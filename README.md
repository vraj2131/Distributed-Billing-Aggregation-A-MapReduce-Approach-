Distributed Billing Aggregation Pipeline
This project implements a scalable billing aggregation system using Apache Spark. We ingest raw API logs, compute per-user total durations and costs for different API tasks via MapReduce functions, and compare a naive single-process baseline with a distributed Spark implementation. Our goal is to provide an end-to-end pipeline that can run locally (via Docker Compose), on Kubernetes (Kind or EKS), or in production on AWSâ€”with dynamic executor allocation and integrated logging.

âš™ï¸ Prerequisites
Docker & Docker Compose

kubectl & Kubernetes (Kind or AWS EKS)

AWS CLI (for EKS mode)

Python 3.8+ with pip

ğŸ“ Setup
Environment Variables
Ensure you have a .env file at the root with all the required settings:

ENVIRONMENT=naive | local | kub | aws

Task rates like RATE_login=0.005

AWS credentials (for EKS mode)

LOG_LEVEL=INFO (optional)

Install Python Dependencies

bash
Copy
Edit
python3 -m venv .venv
source .venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt
ğŸ Naive Approach
Sequential Python script for baseline billing aggregation.

bash
Copy
Edit
time python src/mapreduce_billing/naive_aggregation.py \
     --input-path ./data/api_logs.txt \
     --output-path ./data/billing_naive.txt
ğŸ’» Local Spark Standalone Mode
Run Spark in standalone mode using Docker Compose.

ğŸ”§ Configuration
Set ENVIRONMENT=local in .env

In config.py, set ENVIRONMENT = "local" (line 15)

In spark_job.py, set ENVIRONMENT = "local" (line 24)

In scripts/submit_spark_job.sh, make sure line 125 is:

bash
Copy
Edit
/app/src/mapreduce_billing/spark_job.py \
ğŸ§© Commands
bash
Copy
Edit
chmod +x scripts/submit_spark_job.sh
docker-compose up -d spark-master spark-worker history-server
docker-compose run --rm spark-submit
docker-compose down
â˜¸ï¸ Spark on Kubernetes (Kind)
Run Spark in cluster mode inside a local Kind-based Kubernetes cluster.

ğŸ”§ Configuration
Set ENVIRONMENT=kub in .env

In config.py, set ENVIRONMENT = "kub" (line 15)

In spark_job.py, set ENVIRONMENT = "kub" (line 24)

In scripts/submit_spark_job.sh, make sure line 125 is:

bash
Copy
Edit
local:///app/src/mapreduce_billing/spark_job.py \
ğŸ³ Build & Create Cluster
bash
Copy
Edit
docker build -t distributed-billing-spark:latest .
kind create cluster --name spark-cluster --config config/k8s/kind-spark-cluster.yaml
kind load docker-image distributed-billing-spark:latest --name spark-cluster
kubectl config use-context kind-spark-cluster
kubectl get nodes
ğŸ›¡ï¸ Role Binding & ConfigMap
bash
Copy
Edit
kubectl delete clusterrolebinding spark-sa-admin 2>/dev/null || true
kubectl create clusterrolebinding spark-sa-admin --clusterrole=admin --serviceaccount=default:spark-serviceaccount
kubectl delete configmap app-env
kubectl create configmap app-env --from-env-file=.env
ğŸ§¾ Apply Manifests
bash
Copy
Edit
kubectl apply -f config/k8s/spark-serviceaccount.yaml
kubectl apply -f config/k8s/spark-master-ui-svc.yaml
kubectl apply -f config/k8s/history-server.yaml
kubectl apply -f config/k8s/fluent-bit-config.yaml
kubectl apply -f config/k8s/fluent-bit-daemonset.yaml
kubectl apply -f config/k8s/spark-history-ui-svc.yaml
ğŸŒ Start History Server (New Terminal)
bash
Copy
Edit
kubectl config use-context kind-spark-cluster
kubectl port-forward svc/spark-history-ui 18080:18080
View Spark History UI at: http://localhost:18080

ğŸš€ Run Spark Job
bash
Copy
Edit
kubectl apply -f config/k8s/billing-job.yaml
ğŸ“º Monitor Job & View Logs
bash
Copy
Edit
kubectl get pods --watch
kubectl logs $(kubectl get pods -l job-name=billing-on-demand -o jsonpath='{.items[0].metadata.name}') -c spark-submit
kubectl logs billing-aggregation-$(kubectl get pods -l spark-app-name=billing-aggregation,spark-role=driver -o jsonpath='{.items[0].metadata.name}' | cut -d'-' -f3)-driver -c spark-kubernetes-driver
ğŸ§¹ Cleanup
bash
Copy
Edit
kubectl delete job billing-on-demand
kubectl delete -f config/k8s/history-server.yaml
kubectl delete -f config/k8s/spark-master-ui-svc.yaml
kubectl delete -f config/k8s/spark-serviceaccount.yaml
kubectl delete -f config/k8s/spark-history-ui-svc.yaml
kubectl delete configmap app-env
kind delete cluster --name spark-cluster
â˜ï¸ Spark on AWS EKS (Overview)
For AWS EKS deployment, we created an EKS cluster with a managed node group using the AWS CLI. We built and pushed our Docker image to Amazon ECR, configured kubeconfig for EKS, created a ConfigMap for environment variables, and applied the same billing job manifest. The Spark job ran in cluster mode with dynamic executor allocation, fetched logs from S3, and streamed job progress to CloudWatch. The Spark History UI was accessed via port forwarding.

ğŸ§ª Testing
bash
Copy
Edit
pytest tests/test_naive.py
pytest tests/test_mapreduce.py
ğŸ“ Output
Output files (e.g. billing_results_*.txt) are saved to:

bash
Copy
Edit
./data/results/
Â© 2025 Distributed Billing Aggregation â€“ Scalable billing logic for logs, built to run anywhere.