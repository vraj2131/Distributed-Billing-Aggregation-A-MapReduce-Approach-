# Distributed Billing Aggregation Pipeline

A MapReduce-style billing aggregator for API logs, powered by Apache Spark. Supports both local Docker-Compose testing and Kubernetes cluster mode (Kind/Docker-Desktop or AWS EKS).

---

## ‚öôÔ∏è Prerequisites

* **Docker** & **Docker-Compose** (for local)
* **kubectl** & **Kubernetes** (Docker-Desktop, Kind, or EKS)
* **AWS CLI** & **ECR permissions** (if deploying to EKS)
* **Python 3.8+** with `pip` (for local testing & running scripts)

---

## üìù Setup

### 1. Environment Variables

Copy and customize:

```bash
cp .env.template .env
# Edit .env:
# - ENVIRONMENT=local (or aws)
# - SPARK_MASTER_URL_LOCAL/s3 endpoints
# - AWS_ACCESS_KEY_ID, SECRET, REGION
# - RATE_<task> values
```

### 2. Install Python Dependencies (optional for local testing)

Activate your venv and install:

```bash
python3 -m venv .venv
source .venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt
```

---

## üöÄ Local Testing (Docker-Compose)

1. **Build & launch** everything locally:

   ```bash
   cd docker
   docker-compose up --build
   ```

2. **run\_local.sh** will:

   * Build the Spark image
   * Start a Spark master + worker container
   * Run your Spark job in client mode
   * Tear down when complete

3. **Check output** in `data/billing.txt` (or printed to console).

---

## üê≥ Kubernetes Cluster Mode (Local or AWS)

#### A) Prepare `.env` Secret

```bash
kubectl create secret generic app-env \
  --from-file=.env=./.env
```

#### B) Apply Manifests

```bash
kubectl apply -f configs/k8s/spark-serviceaccount.yaml
kubectl apply -f configs/k8s/fluent-bit-config.yaml
kubectl apply -f configs/k8s/fluent-bit-daemonset.yaml
kubectl apply -f configs/k8s/billing-cronjob.yaml    # scheduled runs
# or on-demand:
kubectl apply -f configs/k8s/billing-job.yaml
```

#### C) Monitor Logs

* **Local**: logs appear via DaemonSet stdout on each node.
* **AWS**: Fluent Bit ships logs to CloudWatch under `/kubernetes/fluent-bit-logs`.

You can also port-forward the Spark UI:

```bash
kubectl get pods --selector=app=spark-driver
kubectl port-forward <driver-pod> 4040:4040
```

---

## üìñ Testing

Run pure-Python tests without Spark:

```bash
pytest tests/test_naive.py
pytest tests/test_mapreduce.py
```

---

## üì¶ Deploy to AWS EKS

1. **Tag & push** Docker image to ECR:

   ```bash

docker build -t \$ECR\_URI\:latest -f docker/spark/Dockerfile .
docker push \$ECR\_URI\:latest

```
2. **Create `app-env`** in EKS (AWS-mode `.env`).
3. **Apply** the same `configs/k8s/` manifests.

Your Spark job will now run on EKS in cluster mode with dynamic allocation.

---

## üîß Further Reading

- Spark on Kubernetes: https://spark.apache.org/docs/latest/running-on-kubernetes.html  
- Fluent Bit ‚Üí CloudWatch: https://docs.fluentbit.io/manual/pipeline/outputs/cloudwatch

---

¬© 2025 Distributed Billing Aggregation

```
