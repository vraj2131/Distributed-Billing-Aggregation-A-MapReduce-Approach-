# configs/k8s/billing-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: billing-on-demand
  namespace: default
spec:
  backoffLimit: 0
  ttlSecondsAfterFinished: 3600
  template:
    spec:
      serviceAccountName: spark-serviceaccount
      restartPolicy: Never
      volumes:
        # Mount your .env as a Secret so the submit script picks up ENV vars
        - name: app-env
          configMap:
            name: app-env
        - name: data
          hostPath:
            path: /Users/vraj21/Desktop/DIS/data
            type: DirectoryOrCreate
        # HostPath for Spark event logs (on your laptop)
        - name: spark-event-logs
          hostPath:
            path: /Users/vraj21/Desktop/DIS/logs/spark-events    # adjust to your host directory
            type: DirectoryOrCreate
        - name: results
          hostPath:
            path: /Users/vraj21/Desktop/DIS/data/results
            type: DirectoryOrCreate
        - name: spark-upload
          emptyDir: {}
      containers:
        - name: spark-submit
          image: distributed-billing-spark:latest
          imagePullPolicy: IfNotPresent
          workingDir: /app
          env:
            - name: SPARK_CONF_spark.kubernetes.container.image
              value: distributed-billing-spark:latest

            - name: SPARK_CONF_spark.kubernetes.authenticate.driver.serviceAccountName
              value: spark-serviceaccount

            - name: SPARK_CONF_spark.kubernetes.file.upload.path
              value: /tmp/spark-upload
          envFrom:
            - configMapRef:
                name: app-env
          volumeMounts:
            - name: data
              mountPath: /app/data
            - name: results
              mountPath: /app/data/results
            # Spark event logs directory
            - name: spark-event-logs
              mountPath: /app/logs/spark-events
            - name: spark-upload
              mountPath: /tmp/spark-upload
          command: ["/bin/bash", "-c", "/app/scripts/submit_spark_job.sh"]
